{"cells":[{"cell_type":"markdown","metadata":{},"source":["> Martin Fridrich, 03/2021\n","\n","# User churn model in e-commerce retail\n","\n","This notebook strives to form a dataset for a user/customer churn prediction problem in an e-commerce environment. It is based on open [Kaggle - Retail rocket dataset](https://www.kaggle.com/retailrocket/ecommerce-dataset).\n","\n","The churn event (explained variable) is a binary indicator of the user's interaction/no interaction during the subsequent month. The user model (explanatory variables) is based on previous user-item interaction/actions on the website, i.e., days from the last session, average session length, etc. The user model consists of six sets of attributes: recency, frequency, monetary, category & item, datetime, and other characteristics. The design of the user model is verified with a machine learning pipeline.\n","\n","The notebook is structured as follows:  \n","\n","1 [Data loading](#data-loading)  \n","2 [Data transformation](#data-transformation)  \n","3 [User churn model & response](#user-churn-model--response)  \n","4 [Modeling](#modeling)  \n","5 [Conclusion](#conclusion)  \n","6 [References](#references)\n","\n","Note: EDA part of the project is limited to the minimum to keep the document straightforward. First of all, let' us check the working directory and load the required libs.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# set options\n","import warnings  \n","warnings.filterwarnings('ignore')\n","import pandas as pd\n","pd.set_option(\"notebook_repr_html\", False)\n","\n","# load libs\n","import numpy as np\n","import networkx as nx\n","import dask.dataframe as dd\n","import datetime\n","import matplotlib.pyplot as plt\n","import re\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{},"source":["# Data loading\n","\n","In the following code chunks we load the `item_properties`, `events`, `category_tree` datasets into the environment. A couple of basic operations such as data union, type conversion, or ensuring datasets integrity are carried out. Memory is freed of temp vars, and resulting data frames can be examined in the console output.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# load in raw data\n","item_properties_1 = pd.read_csv(\"../data/retail-rocket/item_properties_part1.csv\",\n","                               dtype={\"itemid\":np.int32})\n","\n","item_properties_2 = pd.read_csv(\"../data/retail-rocket/item_properties_part2.csv\",\n","                               dtype={\"itemid\":np.int32})\n","\n","events = pd.read_csv(\"../data/retail-rocket/events.csv\",\n","                    dtype={\"visitorid\":np.int32,\n","                           \"transactionid\":np.float32,\n","                           \"itemid\":np.int32})\n","\n","category_tree = pd.read_csv(\"../data/retail-rocket/category_tree.csv\")                           "]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","tags":[],"trusted":true},"outputs":[],"source":["# item properties\n","item_properties = pd.concat([item_properties_1, item_properties_2],\n","                           ignore_index=True)\n","\n","# get only items relevant to events & price + cat properties\n","item_properties = item_properties[item_properties.property.isin([\"categoryid\",\"790\"]) & \n","                                  item_properties.itemid.isin(events.itemid.unique())]\n","\n","# clean-up values & property data types\n","item_properties.value = item_properties.value.str.replace(\"n\",\"\").astype(\"float\")\n","item_properties.property = item_properties.property.map({\"790\":\"price\",\n","                                                         \"categoryid\":\"categoryid\"})\n","\n","# similarily, clean up events\n","events = events[events.itemid.isin(item_properties.itemid.unique())]\n","\n","# del unused vars\n","del item_properties_1, item_properties_2\n","\n","# examine results\n","display(events.info())\n","display(item_properties.info())\n","display(category_tree.info())"]},{"cell_type":"markdown","metadata":{},"source":["Data types seem fine and fit into the memory. There might be some benefits from using the `category` data type; however, current implementation in the `pd` multi-level index does not work correctly with cats :(."]},{"cell_type":"markdown","metadata":{},"source":["# Data transformation\n","\n","Over the next couple of cells, we combine user-events and item-properties in a way that reflects ever-changing properties such a price! As a result, every user-item event is linked with the valid price and category of an item. In the next chuck, we use aggregations, shifts, and some boolean logic to collapse `item_properties`."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["# form item-prop validity df\n","max_timestamp = np.max((events.timestamp.max(),item_properties.timestamp.max()))\n","\n","#\n","item_properties_sorted = item_properties.sort_values(\"timestamp\")\n","\n","# previous property value\n","item_properties_sorted[\"lag_value\"] = item_properties_sorted.groupby([\"itemid\",\n","                                            \"property\"]).value.shift(1) \n","\n","# time of next change (end valid time)\n","item_properties_sorted[\"lead_timestamp\"] = item_properties_sorted.groupby([\"itemid\",\n","                                                \"property\"]).timestamp.shift(-1) \n","\n","# is the observation meaningful change?\n","item_properties_sorted[\"is_change\"] = np.logical_or(item_properties_sorted.lag_value.isna(),\n","                                            item_properties_sorted.lag_value!=\\\n","                                                    item_properties_sorted.value) \n","\n","#\n","item_properties_sorted = item_properties_sorted[item_properties_sorted.is_change]\n","\n","# time of the next change (end valid time)\n","item_properties_sorted[\"lead_timestamp\"] = item_properties_sorted.groupby([\"itemid\",\n","                                                \"property\"]).timestamp.shift(-1)\n","\n","# fill in missing values\n","item_properties_sorted[\"lead_timestamp\"].fillna(max_timestamp, inplace=True)\n","\n","item_properties_sorted[\"lead_timestamp\"] = item_properties_sorted[\"lead_timestamp\"]\\\n","                                                .astype(\"int64\")\n","\n","# rename\n","item_properties_sorted.rename({\"timestamp\":\"valid_start\",\n","                               \"lead_timestamp\":\"valid_end\"},\n","                                axis=1, inplace=True)\n","# filter on cols\n","item_properties = item_properties_sorted.loc[:,(\"valid_start\", \"valid_end\",\n","                                                \"itemid\", \"property\", \"value\")]\n","\n","# add time valid\n","item_properties[\"time_valid\"] = item_properties.valid_end - item_properties.valid_start\n","\n","# clean-up and show res format\n","del item_properties_sorted, max_timestamp\n","item_properties.info()"]},{"cell_type":"markdown","metadata":{},"source":["Following chunks deals with left-joining collapsed `item_properties` on `events`. We ensure that only valid user-event and item-properties are present in the formed dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["# add item property to events df\n","events_enhanced = events.merge(item_properties, on=\"itemid\")\n","\n","events_enhanced = events_enhanced[\n","                    np.logical_and(events_enhanced.timestamp>=events_enhanced.valid_start,\n","                        events_enhanced.timestamp<events_enhanced.valid_end)]\n","\n","events_enhanced = events_enhanced.loc[:,[\"timestamp\", \"visitorid\", \"itemid\",\n","                                         \"event\", \"property\", \"value\"]]\n","\n","events_enhanced = events_enhanced.pivot_table(\n","                        index=[\"timestamp\",\"visitorid\",\"itemid\", \"event\"],\n","                        columns=\"property\", values=\"value\",\n","                        observed=True)\n","\n","events_enhanced.columns = list(events_enhanced.columns)\n","\n","events_enhanced = events_enhanced.reset_index()\n","\n","events_enhanced.rename(index={\"property\":\"index\"},\n","                       inplace=True)\n","\n","del events\n","events_enhanced.isnull().sum()"]},{"cell_type":"markdown","metadata":{},"source":["We see a significant number of missing values within the `categoryid` and `price` columns. To address the problem, we decided to impute the `NaN`s with values valid over the most extended period."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# fill in missing values\n","# get properties valid most of the time\n","top_item_properties = item_properties.groupby([\"itemid\",\"property\",\"value\"],\n","                            as_index=False).time_valid.sum().sort_values(\"time_valid\")\\\n","                            .groupby([\"itemid\",\"property\"]).tail(1)\n","\n","top_item_properties = top_item_properties.pivot_table(index=[\"itemid\"],\n","                                              columns=\"property\", values=\"value\",\n","                                              observed=True).reset_index()\n","\n","# fill where applicable\n","events_enhanced = events_enhanced.merge(top_item_properties, on=\"itemid\")\n","\n","events_enhanced.loc[events_enhanced.categoryid_x.isna(),\n","                        [\"categoryid_x\"]] = events_enhanced[\"categoryid_y\"]\n","\n","events_enhanced.loc[events_enhanced.price_x.isna(),\n","                        [\"price_x\"]] = events_enhanced[\"price_y\"]\n","\n","events_enhanced.rename({\"categoryid_x\":\"categoryid\", \"price_x\":\"price\"},\n","                       axis=1, inplace=True)\n","\n","events_enhanced = events_enhanced.loc[:,[\"timestamp\",\"visitorid\",\"itemid\",\n","                                         \"event\", \"categoryid\", \"price\"]]\n","\n","# check the res\n","events_enhanced.isnull().sum()\n","del top_item_properties, item_properties"]},{"cell_type":"markdown","metadata":{},"source":["Imputation succeeded! Now, let's convert our timestamp `int64` to human-readable format and check the histogram of user-item events over the daily hours."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["# timestamps into datetime objects\n","events_enhanced[\"datetime\"] = pd.to_datetime(events_enhanced.timestamp, unit=\"ms\")-\\\n","                                datetime.timedelta(hours=7)\n","\n","print(\"The observations start at {} and end at {}.\"\\\n","      .format(events_enhanced.datetime.dt.date.min(),\n","              events_enhanced.datetime.dt.date.max()))\n","\n","events_enhanced.datetime.dt.hour.hist(bins=24, density=True, figsize=(10,5));\n","plt.title(\"Hourly distribution of user-item events\");\n","plt.show();"]},{"cell_type":"markdown","metadata":{},"source":["Now, we leverage `category_tree` data to impute low-level categories to respective parent categories. It enables us to track user-item info across parent categories!"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["# par nodes\n","par_nodes = category_tree.categoryid[category_tree.parentid.isnull()].values\n","category_tree = category_tree.dropna()\n","category_tree.columns = [\"from\", \"to\"]\n","category_tree = category_tree.apply(lambda x: x.astype(\"int16\"), axis=1)\n","category_tree.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# closest node func\n","def get_closest_node(node, search_set):\n","    \"\"\" Return the closest node from the search_set.\n","        Parameters> node: list, source node, search_set: set, target nodes\n","        Returns> scalar, the closest node\"\"\"\n","    temp_dict = {key:value for (key, value) in node[1].items() if key in search_set}\n","    par_node = np.nan\n","    if bool(temp_dict):\n","        par_node = min(temp_dict,key=temp_dict.get)\n","    return par_node\n","\n","# get network\n","cat_net = nx.convert_matrix.from_pandas_edgelist(category_tree.dropna(),\n","            source=\"from\", target=\"to\")\n","\n","spl = list(nx.algorithms.shortest_path_length(cat_net))\n","\n","# par dict\n","par_dict = {node[0]:get_closest_node(node, set(par_nodes)) for node in spl}\n","\n","# plot it out\n","plt.figure(figsize=(15,10))\n","plt.axis(\"off\")\n","nx.draw_networkx(cat_net,\n","    pos=nx.spring_layout(cat_net, iterations=75),\n","    node_size=[200 if n in par_nodes else 15 for n in cat_net.nodes],\n","    node_color=[par_dict[n] for n in cat_net.nodes],\n","    cmap=plt.cm.get_cmap('tab20c', 25),\n","    with_labels=False)\n","plt.show();    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# finally remap all categories to respective parents\n","events_enhanced.categoryid = events_enhanced.categoryid.map(par_dict)\n","par_dict = {par_nodes[i] : \"cat\" + str(i) for i in range(len(par_nodes))}\n","events_enhanced.categoryid = events_enhanced.categoryid.map(par_dict)"]},{"cell_type":"markdown","metadata":{},"source":["# User churn model & response\n","\n","In this section, we form user sessions, customer features, and target class. Components are constructed on user data collected within the range `2015-04-01` and `2015-08-20` (inclusive). The observations from `2015-08-20` onwards (last month) are used to create target class labels (visit/no visit).\n","\n","The natural motivation for using session data is the exploitation of higher-level behavior patterns, not only user-item clicks; a similar approach is applied in Berger & Kompan (2019). Consequently, retrieved user-item interactions are aggregated into customer features."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["# sessions\n","sessions = events_enhanced.sort_values([\"visitorid\",\"timestamp\"])\n","\n","sessions[\"row_number\"] = np.arange(len(sessions))\n","\n","sessions[\"vis_diff\"] = np.logical_or(sessions.visitorid.shift(1).isna(),\n","                                     sessions.visitorid.shift(1)!=sessions.visitorid)\n","\n","sessions[\"time_diff\"] = np.logical_or(sessions.datetime.isna(),\n","                                      (sessions.datetime-\n","                                       sessions.datetime.shift(1))\\\n","                                      .astype(\"timedelta64[m]\").astype(\"float\")>15)\n","\n","# session groups\n","groups = sessions.loc[np.logical_or(sessions.vis_diff, sessions.time_diff),:]\n","\n","groups.loc[:,\"row_end\"] = groups.row_number.shift(-1)\n","\n","groups.loc[groups.row_end.isna(),\"row_end\"] = len(groups)+1\n","\n","groups.loc[:,\"sessionid\"] = np.arange(len(groups))\n","\n","groups.rename({\"row_number\":\"row_start\"},\n","             axis=1, inplace=True)\n","               \n","groups = groups.loc[:,[\"visitorid\", \"sessionid\", \"row_start\", \"row_end\"]]\n","\n","# merging it together\n","sessions = sessions.merge(groups, on=\"visitorid\")\n","\n","sessions = sessions.loc[np.logical_and(sessions.row_number>=sessions.row_start,\n","                                       sessions.row_number<sessions.row_end),:]\n","\n","# check res\n","sessions.info()\n","del events_enhanced"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# to improve devtime, sample sessions data\n","# sessions = sessions.sample(frac=.01, random_state=20200702)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def agg_sessions(input_df):\n","    \"\"\"Compute session characteristics such as total number of interactions,\n","    session start, session length, etc.\n","        \n","        Parameters>\n","            input_df: pd.DataFrame, a session data\n","\n","        Returns>\n","            pd.DataFrame, session attributes\"\"\"\n","    \n","    output_dct = {}\n","    output_dct[\"item_n\"] = input_df.itemid.nunique()\n","    output_dct[\"cat_n\"] = input_df.categoryid.nunique()\n","    output_dct[\"int_n\"] = input_df.shape[0]    \n","    output_dct[\"view_n\"] = (input_df.event==\"view\").sum()\n","    output_dct[\"cart_n\"] = (input_df.event==\"addtocart\").sum()\n","    output_dct[\"tran_n\"] = (input_df.event==\"transaction\").sum()    \n","    output_dct[\"view_sp\"] = input_df.loc[(input_df.event==\"view\"),\"price\"].sum()\n","    output_dct[\"cart_sp\"] = input_df.loc[(input_df.event==\"addtocart\"),\"price\"].sum()\n","    output_dct[\"tran_sp\"] = input_df.loc[(input_df.event==\"transaction\"),\"price\"].sum()    \n","    output_dct[\"start_time\"] = input_df.datetime.min()\n","    output_dct[\"end_time\"] = input_df.datetime.max()\n","    output_dct[\"len\"] = (output_dct[\"end_time\"]-output_dct[\"start_time\"])\n","    \n","    return pd.Series(output_dct,\n","                index=output_dct.keys())\n","\n","# meta map\n","agg_sessions_meta = {\"item_n\":np.int32,\n","                    \"cat_n\":np.int16,\n","                    \"int_n\":np.int32,\n","                    \"view_n\":np.int16,\n","                    \"cart_n\":np.int16,\n","                    \"tran_n\":np.int16,\n","                    \"view_sp\":np.float32,\n","                    \"cart_sp\":np.float32,\n","                    \"tran_sp\":np.float32,\n","                    \"start_time\":\"datetime64[s]\",\n","                    \"end_time\":\"datetime64[s]\",\n","                    \"len\":\"timedelta64[s]\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["st = datetime.datetime.now()\n","\n","# dds\n","dd_sessions = dd.from_pandas(sessions, npartitions=8).persist()\n","dd_sessions = dd_sessions.categorize(columns=\"categoryid\")\n","\n","# cats\n","pivcat_sessions = dd_sessions.pivot_table(index=\"sessionid\",\n","                                            columns=\"categoryid\",\n","                                            values=\"visitorid\",\n","                                            aggfunc=\"count\")\n","\n","# group-apply\n","aggregated_sessions = dd_sessions.groupby([\"visitorid\",\"sessionid\"])\\\n","                            .apply(agg_sessions, meta=agg_sessions_meta)\n","\n","# others\n","aggregated_sessions[\"major_spend\"] = aggregated_sessions.tran_sp>\\\n","                            aggregated_sessions.tran_sp.mean()\n","\n","# compute\n","aggregated_sessions = aggregated_sessions.compute(scheduler='processes')\n","pivcat_sessions = pivcat_sessions.compute(scheduler='processes')                                          \n","\n","# put it together\n","aggregated_sessions = aggregated_sessions.reset_index()\n","aggregated_sessions = aggregated_sessions.merge(pivcat_sessions,\n","                            left_on=\"sessionid\", right_on=\"sessionid\")\n","\n","se = datetime.datetime.now()\n","td = (se-st).seconds/60\n","\n","print(\"The transformation took {:.2f} mins on the Dask back-end.\".format(td))\n","del dd_sessions, sessions, pivcat_sessions"]},{"cell_type":"markdown","metadata":{},"source":["The session distribution over time and feature-target split are presented below."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# checkup sessions in train and test splits \n","split_time = aggregated_sessions.end_time.max()-datetime.timedelta(days=31)\n","aggregated_sessions.end_time.hist(bins=(aggregated_sessions.end_time.dt.date.max()-\n","                                        aggregated_sessions.start_time.dt.date.min()).days,\n","                                  figsize=(20,5))\n","plt.axvline(x=split_time,\n","            linestyle =\"--\",\n","            color=\"k\")\n","plt.title(\"Daily sessions over time\");\n","plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def safe_div(a,b):\n","    \"\"\"Fix division by zero.\n","    \n","        Parameters>\n","            a: numeric,numerator\n","            b: numeric, denominator\n","            \n","        Returns>\n","            numeric, division\"\"\"\n","    \n","    res = np.nan\n","    if b!=0:\n","        res = a/b\n","        \n","    return res\n","\n","def agg_customers(input_df, split_time):\n","    \"\"\"Compute customer attributes such as account maturity,\n","    average number of interactions per session, average session length, etc.\n","    \n","        Parameters>        \n","            input_df: pd.DataFrame, a customer data\n","            split_time: datetime64, feature-target split \n","  \n","        Returns>\n","            pd.DataFrame, customer features\"\"\"\n","    \n","    # attributes dict\n","    output_dct = {}\n","    \n","    # recency\n","    input_df = input_df.sort_values(\"start_time\", ascending=True)\n","    ses_rec_diff = (input_df.start_time-input_df.start_time.shift(1)).dt.days\n","    \n","    output_dct[\"ses_rec\"] = (split_time-input_df.end_time.max()).days\n","    output_dct[\"ses_rec_avg\"] = np.nanmean(ses_rec_diff)\n","    output_dct[\"ses_rec_sd\"] = np.nanstd(ses_rec_diff)\n","    output_dct[\"ses_rec_cv\"] = safe_div(output_dct[\"ses_rec_sd\"],\n","                                        output_dct[\"ses_rec_avg\"])\n","    output_dct[\"user_rec\"] = (split_time-input_df.start_time.min()).days   \n","    \n","    # frequency\n","    output_dct[\"ses_n\"] = input_df.shape[0]\n","    output_dct[\"ses_n_r\"] = safe_div(output_dct[\"ses_n\"],\n","                                     output_dct[\"user_rec\"])\n","    output_dct[\"int_n\"] = input_df.int_n.sum()\n","    output_dct[\"int_n_r\"] = safe_div(output_dct[\"int_n\"],\n","                                     output_dct[\"ses_n\"])\n","    output_dct[\"tran_n\"] = input_df.tran_n.sum()\n","    output_dct[\"tran_n_r\"] = safe_div(output_dct[\"tran_n\"],\n","                                     output_dct[\"ses_n\"])\n","    # monetary\n","    output_dct[\"rev_sum\"] = input_df.tran_sp.sum()\n","    output_dct[\"rev_sum_r\"] = safe_div(output_dct[\"rev_sum\"],\n","                                       output_dct[\"ses_n\"])\n","    output_dct[\"major_spend_r\"] = input_df.major_spend.mean()\n","    \n","    # category & item\n","    output_dct[\"int_cat_n_avg\"] = input_df.cat_n.mean()\n","    output_dct[\"int_itm_n_avg\"] = input_df.item_n.mean()\n","    \n","    # date & time\n","    output_dct[\"ses_mo_avg\"] = input_df.start_time.dt.month.mean()\n","    output_dct[\"ses_mo_sd\"] = input_df.start_time.dt.month.std()\n","    output_dct[\"ses_ho_avg\"] = input_df.start_time.dt.hour.mean()\n","    output_dct[\"ses_ho_sd\"] = input_df.start_time.dt.hour.std()\n","    output_dct[\"ses_wknd_r\"] = (input_df.start_time.dt.weekday>4).mean()\n","    \n","    # others\n","    ses_len_d = input_df.len.dt.days.sum() #days\n","    ses_len_m = input_df.len.dt.seconds.sum()/60 #mins\n","    \n","    output_dct[\"ses_len_avg\"] = safe_div(ses_len_m,\n","                                     output_dct[\"ses_n\"])\n","    output_dct[\"time_to_int\"] = safe_div(ses_len_m,\n","                                         output_dct[\"int_n\"])\n","    output_dct[\"time_to_tran\"] = safe_div(ses_len_d,\n","                                          output_dct[\"tran_n\"])\n","    \n","    return pd.Series(output_dct,\n","                index=output_dct.keys())\n","    \n","# meta map\n","agg_customers_meta = {\"ses_rec\":np.float32,                      \n","                      \"ses_rec_avg\":np.float32,\n","                      \"ses_rec_sd\":np.float32,\n","                      \"ses_rec_cv\":np.float32,\n","                      \"user_rec\":np.float32,\n","                      \"ses_n\":np.int32,\n","                      \"ses_n_r\":np.float32,\n","                      \"int_n\":np.int32,\n","                      \"int_n_r\":np.float32,\n","                      \"tran_n\":np.int32,\n","                      \"tran_n_r\":np.float32,\n","                      \"rev_sum\":np.float32,\n","                      \"rev_sum_r\":np.float32,\n","                      \"major_spend_r\":np.float32,                   \n","                      \"int_cat_n_avg\":np.float32,                                   \n","                      \"int_itm_n_avg\":np.float32,                 \n","                      \"ses_mo_avg\":np.float32,                     \n","                      \"ses_mo_sd\":np.float32,\n","                      \"ses_ho_avg\":np.float32,                   \n","                      \"ses_ho_sd\":np.float32,\n","                      \"ses_wknd_r\":np.float32,\n","                      \"ses_len_avg\":np.float32,\n","                      \"time_to_int\":np.float32,\n","                      \"time_to_tran\":np.float32}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["st = datetime.datetime.now()\n","\n","# form the dd\n","dd_customers = dd.from_pandas(aggregated_sessions[aggregated_sessions.end_time<split_time],\n","                              npartitions=8).persist()\n","\n","# group-apply\n","aggregated_customers = dd_customers.groupby([\"visitorid\"]).apply(agg_customers,\n","                            split_time=split_time, meta=agg_customers_meta)\n","\n","aggregated_customers = aggregated_customers.compute(scheduler='processes')\n","\n","# cats & stuffs\n","reg = re.compile(r\"cat[0-9]+\")\n","filter_cats = np.array(list(filter(reg.match, dd_customers.columns)))\n","filter_sort = np.argsort([int(re.search(\"[0-9]+\",c)[0]) for c in filter_cats])\n","filter_cats = list(filter_cats[filter_sort])\n","cat_customers = dd_customers[[\"visitorid\"]+filter_cats].groupby(\"visitorid\").sum()\n","cat_customers = cat_customers.reset_index().compute(scheduler='processes')\n","\n","# others\n","cat_customers = cat_customers.apply(lambda x: pd.to_numeric(x, downcast=\"integer\"))\n","cat_customers.columns =[\"int_\"+c+\"_n\" if c in filter_cats else c for c in cat_customers.columns]\n","\n","# put it together\n","aggregated_customers = aggregated_customers.reset_index()\n","aggregated_customers = aggregated_customers.merge(cat_customers,\n","                            left_on=\"visitorid\", right_on=\"visitorid\")\n","                     \n","se = datetime.datetime.now()\n","td = (se-st).seconds/60\n","\n","print(\"The transformation took {:.2f} mins on the Dask Dask back-end.\".format(td))\n","del dd_customers, cat_customers"]},{"cell_type":"markdown","metadata":{},"source":["Now, we construct target class data frame, eventhough we focus on views only, we allow for multiple class prediction in the future (view/addtocart/purchase). This decision is motivated by massive event imbalance."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# form targets\n","target = aggregated_customers[[\"visitorid\"]].merge(\n","            aggregated_sessions[aggregated_sessions.end_time>=split_time],\n","            how=\"left\", on=\"visitorid\")\n","\n","target.loc[:, \"target_class\"] = 0 # churned\n","target.loc[target.int_n>0, \"target_class\"] = 1\n","target.loc[target.tran_n>0, \"target_class\"] = 2\n","\n","target = target.groupby(\"visitorid\")[\"target_class\"].max().reset_index()\n","del aggregated_sessions"]},{"cell_type":"markdown","metadata":{},"source":["Here we select only recent customers with more then 1 session. Target class vector within the customer features data frame (visit:`0`, no-visit:`1`) is constructed. Moreover, we peek at target class distributions."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# select custs & form the target\n","classification_data = aggregated_customers.merge(target, on=\"visitorid\")\n","filter_class = (classification_data.ses_n>1)&(classification_data.ses_rec<=31)\n","classification_data = classification_data[filter_class]\n","classification_data[\"target_class\"] = [0 if c>0 else 1 for c in\\\n","                                       classification_data.target_class]\n","\n","del aggregated_customers, target\n","classification_data.target_class.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["# fill in blanks & downcast\n","def downcast_dtype(column_series):\n","    \"\"\"Try to infer better dtype.\n","    \n","        Parameters>\n","            column_series: pd.Series, a column to down\n","            \n","        Returns>\n","            pd.Series, converted column\"\"\"        \n","    \n","    conv_dict = {\"int\":\"integer\", \"float\":\"float\"}\n","    for k in conv_dict.keys():\n","        if k in str(column_series.dtype):\n","            return pd.to_numeric(column_series, downcast=conv_dict[k])\n","        \n","classification_data.fillna(-1, inplace=True)\n","classification_data = classification_data.apply(downcast_dtype, axis=0)\n","classification_data.info()"]},{"cell_type":"markdown","metadata":{},"source":["Let'us visually examine relationships amongst several customer features and classes of interest."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["# pp\n","sns.pairplot(classification_data.sample(frac=.1, random_state=20200821),\n","    hue=\"target_class\", vars=[\"user_rec\", \"ses_rec\", \"ses_n\"],\n","    diag_kws=dict(bw=0.75), plot_kws=dict(alpha=0.5), height=3, aspect=1);\n","plt.show();            "]},{"cell_type":"markdown","metadata":{},"source":["We can see that the `user_rec` and `ses_rec` features can somewhat separate the target classes on their own (check respective density plots). Most of the interactions between the features also reveal some expectable patterns (old accounts with short times between sessions tend to be retained)."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#classification_data.to_csv(\"../data/ecom-user-churn-data.csv\", index=False)\n","#classification_data = pd.read_csv(\"../data/ecom-user-churn-data.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["# Modeling\n","\n","The aim of the section is to demonstrate predictive power of the proposed user churn model. Thus, viable machine learning pipeline is designed, fitted, and evaluated"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# modeling libs\n","from sklearn.model_selection import train_test_split\n","from imblearn.pipeline import Pipeline\n","from sklearn.feature_selection import VarianceThreshold, SelectPercentile\n","from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import make_scorer,accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"]},{"cell_type":"markdown","metadata":{},"source":["We utilize stratified 90-10 train-test split."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# split\n","X_train, X_test, y_train, y_test = train_test_split(classification_data.loc[:,\n","                                        \"ses_rec\":\"int_cat24_n\"],\n","                                        classification_data.target_class, test_size=.1,\n","                                        random_state=20200802,\n","                                        stratify=classification_data.target_class)\n","\n","data = {\"train\":{\"X\":X_train, \"y\":y_train},\n","        \"test\":{\"X\":X_test, \"y\":y_test}}\n","\n","#del X_train, X_test, y_train, y_test"]},{"cell_type":"markdown","metadata":{},"source":["The code cell bellow forms the predictive pipeline and sets up the `LogisticRegression` classifier. In the data processing part, features with low variance are removed, and polynomial + interactions are formed. Only ten percent of the most predictive features, evaluated with `f_classif`, is fed into the model."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# pipe def\n","pipe = Pipeline([(\"nzv\", VarianceThreshold()),\n","    (\"scale\",StandardScaler()),\n","    (\"poly\", PolynomialFeatures()),\n","    (\"imb\", RandomUnderSampler()),\n","    (\"dr\", SelectPercentile()),    \n","    (\"clf\", LogisticRegression(max_iter=1000))])\n","\n","# pipe train\n","fit = pipe.fit(data[\"train\"][\"X\"],data[\"train\"][\"y\"])    "]},{"cell_type":"markdown","metadata":{},"source":["It is time to evaluate the previous efforts. Standard classification metrics such as `acc`, `pre`, `rec`, `f1`, or `auc`, are utilized to address the classification performance."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# perf eval func\n","def get_performance_metrics(fit, X, y, clf_name, set_name):\n","    \"\"\"Obtain classification performance metrics on given pipeline & data set.\n","    \n","        Parameters>\n","            fit: sklearn.pipeline.Pipeline, fitted ml pipeline,\n","            X: pd.DataFrame, features\n","            y: pd.Series, target class\n","            clf_name: string, classifier\n","            set_name: string, dataset label\n","            \n","        Returns>\n","            pd.DataFrame, meta & metrics\"\"\"        \n","    \n","    pred = fit.predict(X)\n","    acc = accuracy_score(y, pred)\n","    pre = precision_score(y, pred)\n","    rec = recall_score(y, pred)\n","    f1 = f1_score(y, pred)\n","    auc = roc_auc_score(y,fit.predict_proba(X)[:,1])\n","    \n","    return pd.DataFrame([[clf_name, set_name, acc, pre, rec, f1, auc]],\n","        columns=[\"clf_name\", \"set_name\", \"acc\", \"pre\", \"rec\", \"f1\", \"auc\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# run eval on train-test splits\n","performance_table = pd.concat([get_performance_metrics(fit, dt[\"X\"], dt[\"y\"], \"lr\", d)\n","                    for d, dt in data.items()])\n","# print results\n","performance_table.sort_values([\"set_name\"]).head()"]},{"cell_type":"markdown","metadata":{},"source":["From the table above, we conclude that we can predict if the user will visit the website during the subsequent month. The overall pipeline performance might be addressed with further fine-tuning of the steps & hyperparameters."]},{"cell_type":"markdown","metadata":{},"source":["# Conclusion\n","\n","Over the last couple of cells, we successfully form the user churn model & supporting dataset based on open [Kaggle - Retail rocket dataset](https://www.kaggle.com/retailrocket/ecommerce-dataset). Also, the predictive power of the user churn model is proved with the simple machine learning pipeline. Even though we introduce some unique features, such as parent category interactions, the user model might be extended further. Future work on the user churn prediction might include feature importance estimation, transparent & optimized machine learning pipelines, etc.\n"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"section_ID\"></a>\n","# References\n","Berger & Kompan (2019). User Modeling for Churn Prediction in E-Commerce. Ieee Intelligent Systems, 34(2), 44-52. https://doi.org/10.1109/MIS.2019.2895788  "]},{"cell_type":"markdown","metadata":{},"source":["> Martin Fridrich, 03/2021"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.5 64-bit","name":"python385jvsc74a57bd0badc708db916837e004c5f961f2e0b4e079f3c60a55e9ac72f69e0ccf2a07f68"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"metadata":{"interpreter":{"hash":"badc708db916837e004c5f961f2e0b4e079f3c60a55e9ac72f69e0ccf2a07f68"}}},"nbformat":4,"nbformat_minor":4}